{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure all necessary imports are included\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implementing a simple policy network for tic tac toe that outputs a probability distribution over all moves\n",
    "\n",
    "class SimplePolicyNetwork(nn.Module):\n",
    "    def __init__(self, board_size, num_moves):\n",
    "        \"\"\"\n",
    "        Initializes the Policy Network.\n",
    "        :param board_size: Tuple of the board dimensions, e.g., (19, 19) for Go.\n",
    "        :param num_moves: Total number of possible moves in the game.\n",
    "        \"\"\"\n",
    "        super(SimplePolicyNetwork, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 64, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.fc = nn.Linear(128 * board_size[0] * board_size[1], num_moves)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass of the network.\n",
    "        :param x: Input tensor, the game state.\n",
    "        :return: Probability distribution over all possible moves.\n",
    "        \"\"\"\n",
    "        # Apply two convolutional layers with ReLU activations\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "\n",
    "        # Flatten the output for the fully connected layer\n",
    "        x = x.view(x.size(0), -1)\n",
    "\n",
    "        # Output layer with a softmax to get probabilities\n",
    "        x = self.fc(x)\n",
    "        return F.softmax(x, dim=1)\n",
    "\n",
    "# Example usage\n",
    "board_size = (3, 3)  # For Tic Tac Toe\n",
    "num_moves = board_size[0] * board_size[1]  # Assuming each cell is a possible move\n",
    "model = SimplePolicyNetwork(board_size, num_moves)\n",
    "\n",
    "# Example input: a single game state, with 1 channel, and 19x19 board size\n",
    "# The input should be a 4D tensor: [batch_size, channels, height, width]\n",
    "# Here, batch_size = 1, channels = 1 (just the board, could be more for different game states)\n",
    "game_state = torch.randn(1, 1, board_size[0], board_size[1])\n",
    "# Get the probability distribution over moves\n",
    "probabilities = model(game_state)\n",
    "\n",
    "print(probabilities)  # Each element corresponds to the probability of a move being the best next move\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solving tic-tac-toe using tree search\n",
    "# This is a simple implementation of the minimax algorithm\n",
    "\n",
    "# The game is represented as a 3x3 matrix\n",
    "# 0 represents an empty cell\n",
    "# 1 represents a cell with a cross\n",
    "# 2 represents a cell with a circle\n",
    "\n",
    "class TicTacToe:\n",
    "    def __init__(self):\n",
    "        self.board = np.zeros((3, 3), dtype=int)\n",
    "        self.turn = 1\n",
    "        self.winner = 0\n",
    "\n",
    "    def is_full(self):\n",
    "        return np.all(self.board)\n",
    "\n",
    "    def is_winner(self, player):\n",
    "        for i in range(3):\n",
    "            if np.all(self.board[i] == player) or np.all(self.board[:, i] == player):\n",
    "                return True\n",
    "        if np.all(self.board.diagonal() == player) or np.all(np.fliplr(self.board).diagonal() == player):\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    def is_game_over(self):\n",
    "        # Check if any player has won\n",
    "        for player in [1, 2]:\n",
    "            if self.is_winner(player):\n",
    "                self.winner = player\n",
    "                return True\n",
    "        # If no winner and the board is full, it's a draw\n",
    "        if self.is_full():\n",
    "            self.winner = 0  # Indicate a draw\n",
    "            return True\n",
    "        # Game is not over\n",
    "        return False\n",
    "\n",
    "\n",
    "    def get_valid_moves(self):\n",
    "        return np.argwhere(self.board == 0)\n",
    "    \n",
    "    def get_valid_moves_indices(self):\n",
    "        return np.flatnonzero(self.board == 0)\n",
    "    \n",
    "\n",
    "    def make_move(self, move):\n",
    "        self.board[tuple(move)] = self.turn\n",
    "        self.turn = 3 - self.turn\n",
    "\n",
    "    def make_move_from_index(self, index):\n",
    "        move = np.unravel_index(index, (3, 3))\n",
    "        self.make_move(move)\n",
    "\n",
    "    def undo_move(self, move):\n",
    "        self.board[tuple(move)] = 0\n",
    "        self.turn = 3 - self.turn\n",
    "\n",
    "    def reset(self):\n",
    "        self.board = np.zeros((3, 3), dtype=int)\n",
    "\n",
    "    def __str__(self):\n",
    "        return str(self.board)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# game = TicTacToe()\n",
    "# game.make_move((1,1))\n",
    "# print(game)\n",
    "\n",
    "# game.make_move((2,1))\n",
    "# print(game)\n",
    "\n",
    "# game.make_move((0,1))\n",
    "# print(game)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_board_to_input(board):\n",
    "    \"\"\"\n",
    "    Convert the game board to a tensor suitable for the policy network.\n",
    "    The input is a 4D tensor: [batch_size, channels, height, width].\n",
    "    \"\"\"\n",
    "    # Convert the board to a tensor with shape (1, 1, 3, 3)\n",
    "    # 1 channel, the board's state is represented in a 3x3 grid\n",
    "    board_tensor = torch.tensor(board, dtype=torch.float).unsqueeze(0).unsqueeze(0)\n",
    "    return board_tensor\n",
    "\n",
    "def select_move(probabilities, valid_moves_indices):\n",
    "    \"\"\"\n",
    "    Select the move with the highest probability that is also a valid move.\n",
    "    \"\"\"\n",
    "    # Zero out the probabilities of moves that are not valid\n",
    "    prob_masked = probabilities.clone().detach()\n",
    "    prob_masked[0, np.setdiff1d(np.arange(num_moves), valid_moves_indices)] = 0\n",
    "    # Select the move with the highest probability\n",
    "    move_index = torch.argmax(prob_masked).item()\n",
    "    return move_index\n",
    "\n",
    "# Initialize the TicTacToe game\n",
    "game = TicTacToe()\n",
    "\n",
    "# Initialize the policy network\n",
    "model = SimplePolicyNetwork(board_size, num_moves)\n",
    "\n",
    "# Play until the game is over\n",
    "while not game.is_game_over():\n",
    "    # Convert the current game state to a tensor input for the network\n",
    "    current_state_tensor = convert_board_to_input(game.board)\n",
    "    # Get the probability distribution over moves from the policy network\n",
    "    probabilities = model(current_state_tensor)\n",
    "    # Get valid move indices\n",
    "    valid_moves_indices = game.get_valid_moves_indices()\n",
    "    # Select the move with the highest probability among valid moves\n",
    "    selected_move_index = select_move(probabilities, valid_moves_indices)\n",
    "    # Make the move\n",
    "    game.make_move_from_index(selected_move_index)\n",
    "    # Print the board state\n",
    "    print(game)\n",
    "    print(\"------\")\n",
    "\n",
    "# Check the result\n",
    "if game.winner:\n",
    "    print(f\"Player {game.winner} wins!\")\n",
    "else:\n",
    "    print(\"It's a draw!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import random\n",
    "\n",
    "def play_game_with_random(policy_model, game, random_agent, start_first=True):\n",
    "    game.reset()\n",
    "    # Randomly choose which agent starts\n",
    "    game.turn = random.choice([1, 2])\n",
    "    while not game.is_game_over():\n",
    "        if game.turn == 1:  # Policy network's turn\n",
    "            current_state_tensor = convert_board_to_input(game.board)\n",
    "            probabilities = policy_model(current_state_tensor)\n",
    "            valid_moves_indices = game.get_valid_moves_indices()\n",
    "            selected_move_index = select_move(probabilities, valid_moves_indices)\n",
    "            game.make_move_from_index(selected_move_index)\n",
    "        else:  # Random agent's turn\n",
    "            valid_moves_indices = game.get_valid_moves_indices()\n",
    "            selected_move_index = random_agent.select_move(valid_moves_indices)\n",
    "            game.make_move_from_index(selected_move_index)\n",
    "    return game.winner\n",
    "\n",
    "wins = {1: 0, 2: 0, 0: 0}  # 1: Policy Network, 2: Random Agent, 0: Draw\n",
    "num_simulations = 5000\n",
    "\n",
    "for _ in range(num_simulations):\n",
    "    winner = play_game_with_random(model, game, random_agent, start_first)\n",
    "    wins[winner] += 1\n",
    "    game.reset()  # Reset the game to its initial state after each simulation\n",
    "\n",
    "\n",
    "# Plotting win rates\n",
    "labels = ['Policy Network Wins', 'Random Agent Wins', 'Draws']\n",
    "values = [wins[1], wins[2], wins[0]]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(labels, values, color=['blue', 'red', 'green'])\n",
    "plt.title('Win Rates After 100 Simulations')\n",
    "plt.ylabel('Number of Games')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a function to simulate a game between two random agents\n",
    "def play_game_random_vs_random(game, agent1, agent2):\n",
    "    game.reset()\n",
    "    # Randomly choose which agent starts\n",
    "    game.turn = random.choice([1, 2])\n",
    "    while not game.is_game_over():\n",
    "        if game.turn == 1:  # Random agent 1's turn\n",
    "            valid_moves_indices = game.get_valid_moves_indices()\n",
    "            selected_move_index = agent1.select_move(valid_moves_indices)\n",
    "            game.make_move_from_index(selected_move_index)\n",
    "        else:  # Random agent 2's turn\n",
    "            valid_moves_indices = game.get_valid_moves_indices()\n",
    "            selected_move_index = agent2.select_move(valid_moves_indices)\n",
    "            game.make_move_from_index(selected_move_index)\n",
    "    return game.winner\n",
    "\n",
    "# Initializing the game and two random agents\n",
    "game = TicTacToe()\n",
    "random_agent1 = RandomAgent()\n",
    "random_agent2 = RandomAgent()\n",
    "\n",
    "gameHistory = []\n",
    "\n",
    "# Simulating games\n",
    "wins_random_vs_random = {1: 0, 2: 0, 0: 0}  # 1: Random Agent 1, 2: Random Agent 2, 0: Draw\n",
    "num_simulations_random_vs_random = 5000\n",
    "\n",
    "for _ in range(num_simulations_random_vs_random):\n",
    "    winner = play_game_random_vs_random(game, random_agent1, random_agent2)\n",
    "    wins_random_vs_random[winner] += 1\n",
    "    # Store a copy of the board's state and the winner at this point\n",
    "    final_state = (np.copy(game.board), game.winner)\n",
    "    gameHistory.append(final_state)\n",
    "    game.reset()  # Reset the game to its initial state after each simulation\n",
    "\n",
    "# Plotting win rates for games between two random agents\n",
    "labels_random_vs_random = ['Random Agent 1 Wins', 'Random Agent 2 Wins', 'Draws']\n",
    "values_random_vs_random = [wins_random_vs_random[1], wins_random_vs_random[2], wins_random_vs_random[0]]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(labels_random_vs_random, values_random_vs_random, color=['blue', 'red', 'green'])\n",
    "plt.title('Win Rates After 5000 Simulations: Random vs. Random')\n",
    "plt.ylabel('Number of Games')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_returns(rewards, gamma=1.0):\n",
    "    \"\"\"\n",
    "    Compute returns for each time step, given the rewards\n",
    "    and a discount factor gamma.\n",
    "    \"\"\"\n",
    "    R = 0\n",
    "    returns = []\n",
    "    for r in reversed(rewards):\n",
    "        R = r + gamma * R\n",
    "        returns.insert(0, R)\n",
    "    return returns\n",
    "\n",
    "# Initialize environment, policy network, and optimizer\n",
    "game = TicTacToe()\n",
    "policy_network = SimplePolicyNetwork(board_size=(3, 3), num_moves=9)\n",
    "optimizer = optim.Adam(policy_network.parameters(), lr=0.001)\n",
    "\n",
    "num_episodes = 10000\n",
    "\n",
    "\n",
    "def get_reward(game):\n",
    "    \"\"\"\n",
    "    Defines the reward for the policy network based on the game's outcome.\n",
    "    Assume the policy network always plays as player 1.\n",
    "    \"\"\"\n",
    "    if game.is_winner(1):  # Policy network wins\n",
    "        return 1\n",
    "    elif game.is_winner(2):  # Policy network loses\n",
    "        return -1\n",
    "    return 0  # No reward for intermediate moves\n",
    "\n",
    "\n",
    "win_record = []\n",
    "policy_losses = []  # Store policy losses for plotting\n",
    "episode_rewards = []  # Average reward per episode\n",
    "\n",
    "# Training loop\n",
    "for episode in range(num_episodes):\n",
    "    saved_log_probs = []\n",
    "    rewards = []\n",
    "    game.reset()\n",
    "    while not game.is_game_over():\n",
    "        if game.turn == 1:  # Policy network's turn\n",
    "            state = convert_board_to_input(game.board)\n",
    "            probs = policy_network(state)\n",
    "            action = torch.multinomial(probs, 1).item()  # Sample action\n",
    "            saved_log_probs.append(torch.log(probs.squeeze(0)[action]))\n",
    "            game.make_move_from_index(action)\n",
    "        else:  # Random agent's turn\n",
    "            valid_moves = game.get_valid_moves_indices()\n",
    "            action = np.random.choice(valid_moves)\n",
    "            game.make_move_from_index(action)\n",
    "        reward = get_reward(game)  # Define a suitable reward function\n",
    "        rewards.append(reward)\n",
    "    \n",
    "    # Compute returns\n",
    "    returns = compute_returns(rewards)\n",
    "    returns = torch.tensor(returns)\n",
    "\n",
    "    # Determine the outcome of the episode\n",
    "    win_record.append(game.winner)\n",
    "    episode_rewards.append(np.mean(rewards))\n",
    "    \n",
    "    # Collect individual loss tensors in a list\n",
    "    policy_loss_terms = []\n",
    "    for log_prob, R in zip(saved_log_probs, returns):\n",
    "        loss = -log_prob * R\n",
    "        policy_loss_terms.append(loss.unsqueeze(0))\n",
    "\n",
    "    # Concatenate and sum to compute the total policy loss for the episode\n",
    "    total_policy_loss = torch.cat(policy_loss_terms).sum()\n",
    "    policy_losses.append(total_policy_loss.item())  # Recording scalar loss\n",
    "\n",
    "    # Use total_policy_loss for gradient computation\n",
    "    optimizer.zero_grad()\n",
    "    total_policy_loss.backward()\n",
    "    optimizer.step()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Function to calculate rolling average using np.convolve\n",
    "def calculate_rolling_average(data, window_size=10):\n",
    "    window = np.ones(window_size) / window_size\n",
    "    return np.convolve(data, window, 'valid')\n",
    "\n",
    "# Convert win_record to binary for win, loss, and draw\n",
    "wins_binary = (np.array(win_record) == 1).astype(float)\n",
    "losses_binary = (np.array(win_record) == 2).astype(float)\n",
    "draws_binary = (np.array(win_record) == 0).astype(float)\n",
    "\n",
    "# Calculate rolling averages\n",
    "rolling_avg_win_rate = calculate_rolling_average(wins_binary)\n",
    "rolling_avg_loss_rate = calculate_rolling_average(losses_binary)\n",
    "rolling_avg_draw_rate = calculate_rolling_average(draws_binary)\n",
    "rolling_avg_policy_losses = calculate_rolling_average(policy_losses)\n",
    "rolling_avg_episode_rewards = calculate_rolling_average(episode_rewards)\n",
    "\n",
    "# Plotting\n",
    "window_size = 10  # Adjust window_size if needed\n",
    "\n",
    "plt.figure(figsize=(14, 8))\n",
    "\n",
    "# Win, Loss, Draw Rates\n",
    "plt.subplot(3, 1, 1)\n",
    "episodes_adjusted = np.arange(window_size, len(win_record) + 1)\n",
    "plt.plot(episodes_adjusted, rolling_avg_win_rate, label='Win Rate')\n",
    "plt.plot(episodes_adjusted, rolling_avg_loss_rate, label='Loss Rate')\n",
    "plt.plot(episodes_adjusted, rolling_avg_draw_rate, label='Draw Rate')\n",
    "plt.xlabel('Episodes')\n",
    "plt.ylabel('Rate')\n",
    "plt.title('Performance Over Time (Rolling Avg)')\n",
    "plt.legend()\n",
    "\n",
    "# Policy Loss\n",
    "plt.subplot(3, 1, 2)\n",
    "plt.plot(episodes_adjusted, rolling_avg_policy_losses, label='Policy Loss', color='red')\n",
    "plt.xlabel('Episodes')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Policy Loss Over Time (Rolling Avg)')\n",
    "plt.legend()\n",
    "\n",
    "# Average Rewards\n",
    "plt.subplot(3, 1, 3)\n",
    "plt.plot(episodes_adjusted, rolling_avg_episode_rewards, label='Average Reward', color='green')\n",
    "plt.xlabel('Episodes')\n",
    "plt.ylabel('Average Reward')\n",
    "plt.title('Average Reward Per Episode (Rolling Avg)')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# self play\n",
    "\n",
    "def compute_returns(rewards, gamma=1.0):\n",
    "    \"\"\"\n",
    "    Compute returns for each time step, given the rewards\n",
    "    and a discount factor gamma.\n",
    "    \"\"\"\n",
    "    R = 0\n",
    "    returns = []\n",
    "    for r in reversed(rewards):\n",
    "        R = r + gamma * R\n",
    "        returns.insert(0, R)\n",
    "    return returns\n",
    "\n",
    "# Initialize environment, policy network, and optimizer\n",
    "game = TicTacToe()\n",
    "policy_network = SimplePolicyNetwork(board_size=(3, 3), num_moves=9)\n",
    "optimizer = optim.Adam(policy_network.parameters(), lr=0.001)\n",
    "\n",
    "num_episodes = 10000\n",
    "\n",
    "\n",
    "def get_reward(game):\n",
    "    \"\"\"\n",
    "    Defines the reward for the policy network based on the game's outcome.\n",
    "    Assume the policy network always plays as player 1.\n",
    "    \"\"\"\n",
    "    if game.is_winner(1):  # Policy network wins\n",
    "        return 1\n",
    "    elif game.is_winner(2):  # Policy network loses\n",
    "        return -1\n",
    "    return 0  # No reward for intermediate moves\n",
    "\n",
    "\n",
    "win_record = []\n",
    "policy_losses = []  # Store policy losses for plotting\n",
    "episode_rewards = []  # Average reward per episode\n",
    "\n",
    "# Training loop\n",
    "for episode in range(num_episodes):\n",
    "    saved_log_probs = []\n",
    "    rewards = []\n",
    "    game.reset()\n",
    "    while not game.is_game_over():\n",
    "        if game.turn == 1:  # Policy network's turn\n",
    "            state = convert_board_to_input(game.board)\n",
    "            probs = policy_network(state)\n",
    "            action = torch.multinomial(probs, 1).item()  # Sample action\n",
    "            saved_log_probs.append(torch.log(probs.squeeze(0)[action]))\n",
    "            game.make_move_from_index(action)\n",
    "        else:  # Random agent's turn\n",
    "            valid_moves = game.get_valid_moves_indices()\n",
    "            action = np.random.choice(valid_moves)\n",
    "            game.make_move_from_index(action)\n",
    "        reward = get_reward(game)  # Define a suitable reward function\n",
    "        rewards.append(reward)\n",
    "    \n",
    "    # Compute returns\n",
    "    returns = compute_returns(rewards)\n",
    "    returns = torch.tensor(returns)\n",
    "\n",
    "    # Determine the outcome of the episode\n",
    "    win_record.append(game.winner)\n",
    "    episode_rewards.append(np.mean(rewards))\n",
    "    \n",
    "    # Collect individual loss tensors in a list\n",
    "    policy_loss_terms = []\n",
    "    for log_prob, R in zip(saved_log_probs, returns):\n",
    "        loss = -log_prob * R\n",
    "        policy_loss_terms.append(loss.unsqueeze(0))\n",
    "\n",
    "    # Concatenate and sum to compute the total policy loss for the episode\n",
    "    total_policy_loss = torch.cat(policy_loss_terms).sum()\n",
    "    policy_losses.append(total_policy_loss.item())  # Recording scalar loss\n",
    "\n",
    "    # Use total_policy_loss for gradient computation\n",
    "    optimizer.zero_grad()\n",
    "    total_policy_loss.backward()\n",
    "    optimizer.step()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
